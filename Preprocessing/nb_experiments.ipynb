{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.read_csv('training.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "y_train = training_df['topic']\n",
    "y_test = test_df['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'vect__min_df': (1, 5, 11), 'vect__max_df': (0.75, 0.85, 1.0), 'vect__max_features': (None, 5000, 10000), 'vect__ngram_range': ((1, 1), (1, 3), (2, 3), (3, 6)), 'tfidf__use_idf': (True, False), 'tfidf__sublinear_tf': [True], 'clf__fit_prior': (True, False)}\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 19.6min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 42.6min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 74.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 112.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 159.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2160 out of 2160 | elapsed: 187.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 11234.100s\n",
      "\n",
      "Best score: 0.748\n",
      "Best parameters set:\n",
      "\tclf__fit_prior: False\n",
      "\ttfidf__sublinear_tf: True\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__max_df: 0.75\n",
      "\tvect__max_features: None\n",
      "\tvect__min_df: 5\n",
      "\tvect__ngram_range: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    #'vect__min_df': (1, 3, 5, 11, 17, 31),\n",
    "    'vect__min_df': (1, 5, 11),\n",
    "    'vect__max_df': (0.75, 0.85, 1.0),\n",
    "    #'vect__max_df': (0.65, 0.75, 0.85, 1.0),\n",
    "    'vect__max_features': (None, 5000, 10000),\n",
    "    #'vect__max_features': (None, 5000, 10000, 15000, 25000),\n",
    "    'vect__ngram_range': ((1, 1), (1,3), (2,3), (3,6)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__sublinear_tf': [True],\n",
    "    #'tfidf__sublinear_tf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__fit_prior': (True, False)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(training_df['article_words'], training_df['topic'])\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.752\n"
     ]
    }
   ],
   "source": [
    "#preds = grid_search.predict(test_df['article_words'])\n",
    "acc = grid_search.score(test_df['article_words'], test_df['topic'])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.752\n"
     ]
    }
   ],
   "source": [
    "bestimator = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(2,3), max_df=0.75, min_df=5, max_features=None)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True, sublinear_tf=True)),\n",
    "    ('clf', MultinomialNB(fit_prior=False)),\n",
    "])\n",
    "bestimator.fit(training_df['article_words'], training_df['topic'])\n",
    "print(bestimator.score(test_df['article_words'], test_df['topic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.836\n"
     ]
    }
   ],
   "source": [
    "#data leak\n",
    "#filter model\n",
    "def make_rel(x):\n",
    "    if x != 'IRRELEVANT': return 'RELEVANT'\n",
    "    else: return x\n",
    "    \n",
    "training_rel_df = training_df.copy()\n",
    "test_rel_df = test_df.copy()\n",
    "training_rel_df['topic'] = training_rel_df['topic'].apply(make_rel)\n",
    "test_rel_df['topic'] = test_rel_df['topic'].apply(make_rel)\n",
    "\n",
    "rel_pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(2,3), min_df=6)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True, sublinear_tf=True)),\n",
    "    #('sel', SelectKBest(mutual_info_classif, k=1000)),\n",
    "    ('clf', MultinomialNB(fit_prior=False))\n",
    "])\n",
    "rel_pipe.fit(training_rel_df['article_words'], training_rel_df['topic'])\n",
    "print(rel_pipe.score(test_rel_df['article_words'], test_rel_df['topic']))\n",
    "#experiments with feature selection tended to have poorer results\n",
    "#some studies about this phenomenon in text classification, even weak\n",
    "#components tend to be usefully informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adams\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7905982905982906\n",
      "                                 True                              Pred\n",
      "2                       FOREX MARKETS                     MONEY MARKETS\n",
      "5                       FOREX MARKETS                     FOREX MARKETS\n",
      "7                              SPORTS                            SPORTS\n",
      "12                             SPORTS                            SPORTS\n",
      "15                      MONEY MARKETS                     MONEY MARKETS\n",
      "17                     SHARE LISTINGS                    SHARE LISTINGS\n",
      "19                             SPORTS                            SPORTS\n",
      "24                      FOREX MARKETS                     FOREX MARKETS\n",
      "25   BIOGRAPHIES PERSONALITIES PEOPLE        ARTS CULTURE ENTERTAINMENT\n",
      "28                      FOREX MARKETS                     MONEY MARKETS\n",
      "29                      FOREX MARKETS                     FOREX MARKETS\n",
      "30                      MONEY MARKETS                     MONEY MARKETS\n",
      "33                      MONEY MARKETS                     MONEY MARKETS\n",
      "35                             SPORTS                            SPORTS\n",
      "38                      FOREX MARKETS                     FOREX MARKETS\n",
      "40                             SPORTS                            SPORTS\n",
      "41                      MONEY MARKETS                     FOREX MARKETS\n",
      "47                      MONEY MARKETS                     MONEY MARKETS\n",
      "49                      MONEY MARKETS                     MONEY MARKETS\n",
      "50                      FOREX MARKETS                     FOREX MARKETS\n",
      "52                      MONEY MARKETS                     MONEY MARKETS\n",
      "53                      FOREX MARKETS                     MONEY MARKETS\n",
      "54                      MONEY MARKETS                     FOREX MARKETS\n",
      "58                            DEFENCE                           DEFENCE\n",
      "59                      MONEY MARKETS                     MONEY MARKETS\n",
      "61                     SHARE LISTINGS                    SHARE LISTINGS\n",
      "64                      MONEY MARKETS                     FOREX MARKETS\n",
      "67                             SPORTS                            SPORTS\n",
      "68                             SPORTS                            SPORTS\n",
      "69                      FOREX MARKETS                     FOREX MARKETS\n",
      "..                                ...                               ...\n",
      "441                            SPORTS                            SPORTS\n",
      "442                     FOREX MARKETS                     FOREX MARKETS\n",
      "443                     MONEY MARKETS                     FOREX MARKETS\n",
      "446                            HEALTH                            HEALTH\n",
      "451        ARTS CULTURE ENTERTAINMENT        ARTS CULTURE ENTERTAINMENT\n",
      "455  BIOGRAPHIES PERSONALITIES PEOPLE  BIOGRAPHIES PERSONALITIES PEOPLE\n",
      "460                     FOREX MARKETS                     FOREX MARKETS\n",
      "462                     MONEY MARKETS                     MONEY MARKETS\n",
      "463                            SPORTS                            SPORTS\n",
      "464                     FOREX MARKETS                     FOREX MARKETS\n",
      "466                     MONEY MARKETS                     MONEY MARKETS\n",
      "471                    SHARE LISTINGS                     MONEY MARKETS\n",
      "474                     FOREX MARKETS                     FOREX MARKETS\n",
      "476                     FOREX MARKETS                     FOREX MARKETS\n",
      "477                            HEALTH                            HEALTH\n",
      "478                            SPORTS                            SPORTS\n",
      "480                            SPORTS                            SPORTS\n",
      "481                            HEALTH                            HEALTH\n",
      "482  BIOGRAPHIES PERSONALITIES PEOPLE  BIOGRAPHIES PERSONALITIES PEOPLE\n",
      "483                     MONEY MARKETS                     MONEY MARKETS\n",
      "485                     MONEY MARKETS                     FOREX MARKETS\n",
      "486                           DEFENCE                           DEFENCE\n",
      "487  BIOGRAPHIES PERSONALITIES PEOPLE  BIOGRAPHIES PERSONALITIES PEOPLE\n",
      "489                     MONEY MARKETS                     MONEY MARKETS\n",
      "491                            SPORTS                            SPORTS\n",
      "493                  DOMESTIC MARKETS                           DEFENCE\n",
      "494                     MONEY MARKETS                     MONEY MARKETS\n",
      "496                            SPORTS                            SPORTS\n",
      "497                     MONEY MARKETS                     MONEY MARKETS\n",
      "498                    SHARE LISTINGS                    SHARE LISTINGS\n",
      "\n",
      "[234 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#relevancy differentiability\n",
    "training_cat_df = training_df.copy()\n",
    "test_cat_df = test_df.copy()\n",
    "\n",
    "training_cat_df = training_cat_df[training_cat_df.topic != 'IRRELEVANT']\n",
    "test_cat_df = test_cat_df[test_cat_df.topic != 'IRRELEVANT']\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,3), min_df=7)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True, sublinear_tf=True)),\n",
    "    #('sel', SelectKBest(mututal_info_classif, k=100)),\n",
    "    ('clf', MultinomialNB(alpha=0, fit_prior=False))\n",
    "])\n",
    "\n",
    "cat_pipe.fit(training_cat_df['article_words'], training_cat_df['topic'])\n",
    "print(cat_pipe.score(test_cat_df['article_words'], test_cat_df['topic']))\n",
    "preds = cat_pipe.predict(test_cat_df['article_words'])\n",
    "report_df = pd.DataFrame({'True':test_cat_df['topic'], 'Pred':preds})\n",
    "print(report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7094017094017094\n",
      "              True           Pred\n",
      "2    FOREX MARKETS  MONEY MARKETS\n",
      "5    FOREX MARKETS  FOREX MARKETS\n",
      "15   MONEY MARKETS  MONEY MARKETS\n",
      "24   FOREX MARKETS  FOREX MARKETS\n",
      "28   FOREX MARKETS  MONEY MARKETS\n",
      "29   FOREX MARKETS  FOREX MARKETS\n",
      "30   MONEY MARKETS  MONEY MARKETS\n",
      "33   MONEY MARKETS  MONEY MARKETS\n",
      "38   FOREX MARKETS  FOREX MARKETS\n",
      "41   MONEY MARKETS  FOREX MARKETS\n",
      "47   MONEY MARKETS  MONEY MARKETS\n",
      "49   MONEY MARKETS  MONEY MARKETS\n",
      "50   FOREX MARKETS  FOREX MARKETS\n",
      "52   MONEY MARKETS  MONEY MARKETS\n",
      "53   FOREX MARKETS  MONEY MARKETS\n",
      "54   MONEY MARKETS  FOREX MARKETS\n",
      "59   MONEY MARKETS  MONEY MARKETS\n",
      "64   MONEY MARKETS  FOREX MARKETS\n",
      "69   FOREX MARKETS  FOREX MARKETS\n",
      "71   MONEY MARKETS  FOREX MARKETS\n",
      "76   FOREX MARKETS  FOREX MARKETS\n",
      "83   FOREX MARKETS  FOREX MARKETS\n",
      "85   MONEY MARKETS  MONEY MARKETS\n",
      "86   MONEY MARKETS  MONEY MARKETS\n",
      "87   FOREX MARKETS  FOREX MARKETS\n",
      "88   MONEY MARKETS  MONEY MARKETS\n",
      "93   MONEY MARKETS  MONEY MARKETS\n",
      "98   MONEY MARKETS  MONEY MARKETS\n",
      "105  FOREX MARKETS  MONEY MARKETS\n",
      "117  MONEY MARKETS  MONEY MARKETS\n",
      "..             ...            ...\n",
      "354  FOREX MARKETS  MONEY MARKETS\n",
      "359  MONEY MARKETS  MONEY MARKETS\n",
      "362  MONEY MARKETS  MONEY MARKETS\n",
      "365  FOREX MARKETS  MONEY MARKETS\n",
      "368  FOREX MARKETS  FOREX MARKETS\n",
      "374  MONEY MARKETS  FOREX MARKETS\n",
      "388  FOREX MARKETS  MONEY MARKETS\n",
      "391  MONEY MARKETS  MONEY MARKETS\n",
      "392  FOREX MARKETS  MONEY MARKETS\n",
      "393  MONEY MARKETS  MONEY MARKETS\n",
      "397  MONEY MARKETS  MONEY MARKETS\n",
      "400  MONEY MARKETS  MONEY MARKETS\n",
      "401  FOREX MARKETS  MONEY MARKETS\n",
      "415  MONEY MARKETS  MONEY MARKETS\n",
      "423  MONEY MARKETS  FOREX MARKETS\n",
      "437  MONEY MARKETS  MONEY MARKETS\n",
      "438  MONEY MARKETS  MONEY MARKETS\n",
      "442  FOREX MARKETS  FOREX MARKETS\n",
      "443  MONEY MARKETS  FOREX MARKETS\n",
      "460  FOREX MARKETS  FOREX MARKETS\n",
      "462  MONEY MARKETS  MONEY MARKETS\n",
      "464  FOREX MARKETS  FOREX MARKETS\n",
      "466  MONEY MARKETS  MONEY MARKETS\n",
      "474  FOREX MARKETS  FOREX MARKETS\n",
      "476  FOREX MARKETS  FOREX MARKETS\n",
      "483  MONEY MARKETS  MONEY MARKETS\n",
      "485  MONEY MARKETS  FOREX MARKETS\n",
      "489  MONEY MARKETS  MONEY MARKETS\n",
      "494  MONEY MARKETS  MONEY MARKETS\n",
      "497  MONEY MARKETS  MONEY MARKETS\n",
      "\n",
      "[117 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adams\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "training_fin_df = training_df.copy()\n",
    "test_fin_df = test_df.copy()\n",
    "\n",
    "training_fin_df = training_fin_df[(training_fin_df.topic == 'FOREX MARKETS') | (training_fin_df.topic == 'MONEY MARKETS')]\n",
    "test_fin_df = test_fin_df[(test_fin_df.topic == 'FOREX MARKETS') | (test_fin_df.topic == 'MONEY MARKETS')]\n",
    "\n",
    "def get_text_length(x):\n",
    "    return np.array([len(t) for t in x]).reshape(-1, 1)\n",
    "\n",
    "word_pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,3), min_df=7)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True, sublinear_tf=True)),\n",
    "    #('sel', SelectKBest(mututal_info_classif, k=100)),\n",
    "    #('clf', MultinomialNB(alpha=0, fit_prior=False))\n",
    "])\n",
    "\n",
    "\n",
    "fin_pipe = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', word_pipe),\n",
    "        ('length', Pipeline([\n",
    "            ('count', FunctionTransformer(get_text_length, validate=False)),\n",
    "        ]))\n",
    "    ])),\n",
    "    ('clf', MultinomialNB(alpha=0, fit_prior=False))])\n",
    "\n",
    "fin_pipe.fit(training_fin_df['article_words'], training_fin_df['topic'])\n",
    "print(cat_pipe.score(test_fin_df['article_words'], test_fin_df['topic']))\n",
    "preds = cat_pipe.predict(test_fin_df['article_words'])\n",
    "report_df = pd.DataFrame({'True':test_fin_df['topic'], 'Pred':preds})\n",
    "print(report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.762\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "y=test_df['topic']\n",
    "for text in test_df['article_words']:\n",
    "    text_wrap = [text]\n",
    "    pred = rel_pipe.predict(text_wrap)[0]\n",
    "    if pred == 'RELEVANT': pred = cat_pipe.predict(text_wrap)[0]\n",
    "    preds.append(pred)\n",
    "#print(preds[0])\n",
    "acc = (preds == y).mean()\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 True                              Pred\n",
      "0                          IRRELEVANT                        IRRELEVANT\n",
      "1                          IRRELEVANT                        IRRELEVANT\n",
      "2                       FOREX MARKETS                     MONEY MARKETS\n",
      "3                          IRRELEVANT                        IRRELEVANT\n",
      "4                          IRRELEVANT                        IRRELEVANT\n",
      "5                       FOREX MARKETS                     FOREX MARKETS\n",
      "6                          IRRELEVANT                        IRRELEVANT\n",
      "7                              SPORTS                            SPORTS\n",
      "8                          IRRELEVANT                     MONEY MARKETS\n",
      "9                          IRRELEVANT                        IRRELEVANT\n",
      "10                         IRRELEVANT                        IRRELEVANT\n",
      "11                         IRRELEVANT                        IRRELEVANT\n",
      "12                             SPORTS                            SPORTS\n",
      "13                         IRRELEVANT                            SPORTS\n",
      "14                         IRRELEVANT                        IRRELEVANT\n",
      "15                      MONEY MARKETS                     MONEY MARKETS\n",
      "16                         IRRELEVANT                        IRRELEVANT\n",
      "17                     SHARE LISTINGS                        IRRELEVANT\n",
      "18                         IRRELEVANT                        IRRELEVANT\n",
      "19                             SPORTS                            SPORTS\n",
      "20                         IRRELEVANT                        IRRELEVANT\n",
      "21                         IRRELEVANT                        IRRELEVANT\n",
      "22                         IRRELEVANT                        IRRELEVANT\n",
      "23                         IRRELEVANT                        IRRELEVANT\n",
      "24                      FOREX MARKETS                     FOREX MARKETS\n",
      "25   BIOGRAPHIES PERSONALITIES PEOPLE        ARTS CULTURE ENTERTAINMENT\n",
      "26                         IRRELEVANT                        IRRELEVANT\n",
      "27                         IRRELEVANT                     MONEY MARKETS\n",
      "28                      FOREX MARKETS                     MONEY MARKETS\n",
      "29                      FOREX MARKETS                     FOREX MARKETS\n",
      "..                                ...                               ...\n",
      "470                        IRRELEVANT                        IRRELEVANT\n",
      "471                    SHARE LISTINGS                        IRRELEVANT\n",
      "472                        IRRELEVANT                        IRRELEVANT\n",
      "473                        IRRELEVANT                        IRRELEVANT\n",
      "474                     FOREX MARKETS                     FOREX MARKETS\n",
      "475                        IRRELEVANT                        IRRELEVANT\n",
      "476                     FOREX MARKETS                     FOREX MARKETS\n",
      "477                            HEALTH                            HEALTH\n",
      "478                            SPORTS                            SPORTS\n",
      "479                        IRRELEVANT                        IRRELEVANT\n",
      "480                            SPORTS                            SPORTS\n",
      "481                            HEALTH                            HEALTH\n",
      "482  BIOGRAPHIES PERSONALITIES PEOPLE                        IRRELEVANT\n",
      "483                     MONEY MARKETS                     MONEY MARKETS\n",
      "484                        IRRELEVANT                     MONEY MARKETS\n",
      "485                     MONEY MARKETS                     FOREX MARKETS\n",
      "486                           DEFENCE                           DEFENCE\n",
      "487  BIOGRAPHIES PERSONALITIES PEOPLE  BIOGRAPHIES PERSONALITIES PEOPLE\n",
      "488                        IRRELEVANT                        IRRELEVANT\n",
      "489                     MONEY MARKETS                     MONEY MARKETS\n",
      "490                        IRRELEVANT                        IRRELEVANT\n",
      "491                            SPORTS                            SPORTS\n",
      "492                        IRRELEVANT                        IRRELEVANT\n",
      "493                  DOMESTIC MARKETS                        IRRELEVANT\n",
      "494                     MONEY MARKETS                     MONEY MARKETS\n",
      "495                        IRRELEVANT                        IRRELEVANT\n",
      "496                            SPORTS                            SPORTS\n",
      "497                     MONEY MARKETS                     MONEY MARKETS\n",
      "498                    SHARE LISTINGS                        IRRELEVANT\n",
      "499                        IRRELEVANT                     FOREX MARKETS\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "0.762\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "      ARTS CULTURE ENTERTAINMENT       0.25      0.33      0.29         3\n",
      "BIOGRAPHIES PERSONALITIES PEOPLE       0.71      0.33      0.45        15\n",
      "                         DEFENCE       1.00      0.31      0.47        13\n",
      "                DOMESTIC MARKETS       0.00      0.00      0.00         2\n",
      "                   FOREX MARKETS       0.54      0.62      0.58        48\n",
      "                          HEALTH       0.78      0.50      0.61        14\n",
      "                      IRRELEVANT       0.85      0.85      0.85       266\n",
      "                   MONEY MARKETS       0.54      0.71      0.62        69\n",
      "          SCIENCE AND TECHNOLOGY       0.00      0.00      0.00         3\n",
      "                  SHARE LISTINGS       0.00      0.00      0.00         7\n",
      "                          SPORTS       0.94      1.00      0.97        60\n",
      "\n",
      "                       micro avg       0.76      0.76      0.76       500\n",
      "                       macro avg       0.51      0.42      0.44       500\n",
      "                    weighted avg       0.76      0.76      0.75       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#voter\n",
    "preds = []\n",
    "y=test_df['topic']\n",
    "for text in test_df['article_words']:\n",
    "    text_wrap = [text]\n",
    "    pred = rel_pipe.predict(text_wrap)[0]\n",
    "    pred_prob1 = rel_pipe.predict_proba(text_wrap)[0]\n",
    "    pred_prob1 = max(pred_prob1)\n",
    "    pred_prob2 = cat_pipe.predict_proba(text_wrap)[0]\n",
    "    pred_prob2 = max(pred_prob2)\n",
    "    if pred == 'RELEVANT': pred = cat_pipe.predict(text_wrap)[0]\n",
    "    #elif pred_prob2 > pred_prob1: print(cat_pipe.predict(text_wrap)[0], pred_prob2, '>', pred_prob1)\n",
    "    preds.append(pred)\n",
    "#print(preds[0])\n",
    "acc = (preds == y).mean()\n",
    "report_df = pd.DataFrame({'True':y, 'Pred':preds})\n",
    "print(report_df)\n",
    "print(acc)\n",
    "print(classification_report(y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35823\n",
      "[2.38405306e-02 1.17232812e-03 5.92409583e-02 ... 0.00000000e+00\n",
      " 3.99161691e-05 3.99161691e-05]\n",
      "[0.01891437 0.00069765 0.05654598 ... 0.         0.         0.        ]\n",
      "[0.02549916 0.00128358 0.00746048 ... 0.         0.         0.        ]\n",
      "[7.63176384e-03 5.42080874e-04 2.31130935e-02 ... 2.59036476e-05\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "[0.00783205 0.         0.01235197 ... 0.         0.         0.        ]\n",
      "[0.00885717 0.00061149 0.01725865 ... 0.         0.         0.        ]\n",
      "[0.00167749 0.00025989 0.0063773  ... 0.         0.         0.        ]\n",
      "[0.00936375 0.00098046 0.012028   ... 0.         0.         0.        ]\n",
      "[0.0129255  0.         0.01757448 ... 0.         0.         0.        ]\n",
      "[0.004762   0.00035784 0.01321229 ... 0.         0.         0.        ]\n",
      "[0.01181187 0.00057624 0.0091316  ... 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "#custom vectorize\n",
    "#naive scorer model...not really probabilities but class dependent score\n",
    "corpus_indexer = {}\n",
    "corpus_index = 0\n",
    "for sentence in training_df['article_words']:\n",
    "    words = sentence.split(',')\n",
    "    for word in words:\n",
    "        if word not in corpus_indexer:\n",
    "            corpus_indexer[word] = corpus_index\n",
    "            corpus_index += 1\n",
    "\n",
    "print(corpus_index)\n",
    "\n",
    "#probably dont need this intermediate step/datastructure\n",
    "class_data = {}\n",
    "for row in training_df.itertuples():\n",
    "    words = row.article_words.split(',')\n",
    "    topic = row.topic\n",
    "    if topic not in class_data:\n",
    "        class_data[topic] = []\n",
    "    doc_vector = np.zeros((corpus_index))\n",
    "    for word in words:\n",
    "        index = corpus_indexer[word]\n",
    "        doc_vector[index] += 1\n",
    "    class_data[topic].append(doc_vector)\n",
    "\n",
    "#do numpy stuff\n",
    "for topic, matrix in class_data.items():\n",
    "    matrix = np.vstack(matrix)\n",
    "    matrix = matrix/np.linalg.norm(matrix, axis=1, keepdims=True)\n",
    "    class_data[topic] = matrix.mean(0)\n",
    "    \n",
    "for topic, vector in class_data.items():\n",
    "    print(vector)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.636\n"
     ]
    }
   ],
   "source": [
    "def my_vectorizer(words, indexer, size):\n",
    "    vec = np.zeros((size))\n",
    "    for word in words:\n",
    "        if word not in indexer: continue\n",
    "        index = indexer[word]\n",
    "        vec[index] += 1\n",
    "    return vec/np.linalg.norm(vec, keepdims=True)\n",
    "\n",
    "def my_classify(model, vec):\n",
    "    classes = list(model.keys())\n",
    "    num_classes = len(classes)\n",
    "    vals = np.zeros((num_classes))\n",
    "    for i, k in enumerate(classes):\n",
    "        model_vec = model[k]\n",
    "        vals[i] = np.sqrt(np.mean((model_vec-vec)**2))\n",
    "    c = np.argmin(vals)\n",
    "    return classes[c]\n",
    "\n",
    "def accuracy(preds, true):\n",
    "    size = len(preds)\n",
    "    total = 0\n",
    "    for i, v in enumerate(true):\n",
    "        if preds[i] == v: total += 1\n",
    "    return total/size\n",
    "\n",
    "preds = []\n",
    "for sentence in test_df['article_words']:\n",
    "    words = sentence.split(',')\n",
    "    vec = my_vectorizer(words, corpus_indexer, corpus_index)\n",
    "    preds.append(my_classify(class_data, vec))\n",
    "\n",
    "acc = accuracy(preds, test_df['topic'])\n",
    "print(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68\n"
     ]
    }
   ],
   "source": [
    "#a bit confused how test data is transformed...is term frequency within a doc calculated for test data\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_features = vectorizer.fit_transform(training_df['article_words'])\n",
    "test_features = vectorizer.transform(test_df['article_words'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_features, y_train)\n",
    "predictions = classifier.predict(test_features)\n",
    "acc = np.mean(predictions == y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.642\n"
     ]
    }
   ],
   "source": [
    "#a bit confused how test data is transformed...is term frequency within a doc calculated for test data\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "train_features = vectorizer.fit_transform(training_df['article_words'])\n",
    "test_features = vectorizer.transform(test_df['article_words'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_features, y_train)\n",
    "predictions = classifier.predict(test_features)\n",
    "acc = np.mean(predictions == y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.646\n"
     ]
    }
   ],
   "source": [
    "#a bit confused how test data is transformed...is term frequency within a doc calculated for test data\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2))\n",
    "train_features = vectorizer.fit_transform(training_df['article_words'])\n",
    "test_features = vectorizer.transform(test_df['article_words'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_features, y_train)\n",
    "predictions = classifier.predict(test_features)\n",
    "acc = np.mean(predictions == y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.636\n"
     ]
    }
   ],
   "source": [
    "#a bit confused how test data is transformed...is term frequency within a doc calculated for test data\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,5))\n",
    "train_features = vectorizer.fit_transform(training_df['article_words'])\n",
    "test_features = vectorizer.transform(test_df['article_words'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_features, y_train)\n",
    "predictions = classifier.predict(test_features)\n",
    "acc = np.mean(predictions == y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.554\n"
     ]
    }
   ],
   "source": [
    "#a bit confused how test data is transformed...is term frequency within a doc calculated for test data\n",
    "vectorizer = TfidfVectorizer(ngram_range=(10,10))\n",
    "train_features = vectorizer.fit_transform(training_df['article_words'])\n",
    "test_features = vectorizer.transform(test_df['article_words'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_features, y_train)\n",
    "predictions = classifier.predict(test_features)\n",
    "acc = np.mean(predictions == y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.728\n"
     ]
    }
   ],
   "source": [
    "#might be good to get rid of irrelevant class...bias towards it especially since large class\n",
    "#term or document frequency?\n",
    "#a bit confused how test data is transformed...is term frequency within a doc calculated for test data\n",
    "vectorizer = CountVectorizer()\n",
    "train_features = vectorizer.fit_transform(training_df['article_words'])\n",
    "test_features = vectorizer.transform(test_df['article_words'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_features, y_train)\n",
    "predictions = classifier.predict(test_features)\n",
    "acc = np.mean(predictions == y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "train_features = vectorizer.fit_transform(training_df['article_words'])\n",
    "test_features = vectorizer.transform(test_df['article_words'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_features, y_train)\n",
    "predictions = classifier.predict(test_features)\n",
    "acc = np.mean(predictions == y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.732\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "train_features = vectorizer.fit_transform(training_df['article_words'])\n",
    "test_features = vectorizer.transform(test_df['article_words'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_features, y_train)\n",
    "predictions = classifier.predict(test_features)\n",
    "acc = np.mean(predictions == y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,5))\n",
    "train_features = vectorizer.fit_transform(training_df['article_words'])\n",
    "test_features = vectorizer.transform(test_df['article_words'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_features, y_train)\n",
    "predictions = classifier.predict(test_features)\n",
    "acc = np.mean(predictions == y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "train_features = vectorizer.fit_transform(training_df['article_words'])\n",
    "test_features = vectorizer.transform(test_df['article_words'])\n",
    "classifier = MultinomialNB(fit_prior=False)\n",
    "classifier.fit(train_features, y_train)\n",
    "predictions = classifier.predict(test_features)\n",
    "acc = np.mean(predictions == y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.736\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "train_features = vectorizer.fit_transform(training_df['article_words'])\n",
    "test_features = vectorizer.transform(test_df['article_words'])\n",
    "classifier = MultinomialNB(fit_prior=False)\n",
    "classifier.fit(train_features, y_train)\n",
    "predictions = classifier.predict(test_features)\n",
    "acc = np.mean(predictions == y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "train_features = vectorizer.fit_transform(training_df['article_words'])\n",
    "test_features = vectorizer.transform(test_df['article_words'])\n",
    "classifier = MultinomialNB(fit_prior=False)\n",
    "classifier.fit(train_features, y_train)\n",
    "predictions = classifier.predict(test_features)\n",
    "acc = np.mean(predictions == y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,3), use_idf=False, sublinear_tf=True)\n",
    "train_features = vectorizer.fit_transform(training_df['article_words'])\n",
    "test_features = vectorizer.transform(test_df['article_words'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_features, y_train)\n",
    "predictions = classifier.predict(test_features)\n",
    "acc = np.mean(predictions == y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work still needs to be done with the actual feature metric being used some combination of its frequency within a document and the differentiability of that word in a class compared to others...that might be what feature selection does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using feature selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
